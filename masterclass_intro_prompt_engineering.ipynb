{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD88AKHDBmf7"
      },
      "source": [
        "# Tech Venture Bootcamp - Introduction to Prompt Engineering\n",
        "\n",
        "Prof. Alberto Mart√≠n Izquierdo\n",
        "\n",
        "Prof. Santiago Gil Begu√©\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1wIE22tjli0_rg27P_H1aLwWCt9XgQp23\" alt=\"IE\" width=\"150\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MeTaOl24xJB"
      },
      "source": [
        "## üîê Setting Up Your OpenAI API Access\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2KmrZTd48Ao"
      },
      "source": [
        "Before interacting with the OpenAI API from Python, you need to configure your account, billing, and authentication. Follow the steps below to ensure everything works smoothly.\n",
        "\n",
        "1. Create an OpenAI Account\n",
        "If you don't already have one, sign up here:\n",
        "üëâ https://platform.openai.com/docs/overview\n",
        "\n",
        "2. Upgrade to a Paid Plan\n",
        "API usage requires a minimum prepaid balance of $5 USD.\n",
        "You can upgrade your plan here:\n",
        "üëâ https://platform.openai.com/settings/organization/billing/overview\n",
        "\n",
        "3. Set a Usage Limit (Recommended)\n",
        "To avoid unexpected charges, configure spending limits for your organization:\n",
        "üëâ https://platform.openai.com/settings/organization/limits\n",
        "\n",
        "4. Generate an API Key\n",
        "Create a new API key that your application will use to authenticate requests:\n",
        "üëâ https://platform.openai.com/settings/organization/api-keys ‚Üí Create new secret key\n",
        "\n",
        "Important notes:\n",
        "\n",
        "- You may be asked to create a project first and assign the API key to it.\n",
        "- If your API key was created before you upgraded to a paid plan, you may need to delete it and generate a new one (common cause of quota errors). Relevant reference: https://stackoverflow.com/questions/75898276/openai-api-error-429-you-exceeded-your-current-quota-please-check-your-plan-a\n",
        "\n",
        "\n",
        "5. Keep Your Key Secure\n",
        "API keys are sensitive credentials. Never share them publicly or store them in version control. Use environment variables or a .env file instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQW6YomX5TD8"
      },
      "source": [
        "## üöÄ Getting Started: Your First Interactions with the API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTPlphfrp1ER"
      },
      "source": [
        "You will learn how to create a minimal ‚Äúhello world‚Äù completion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjawOuB6tFI0"
      },
      "source": [
        "### Basic syntax and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hu_scJ23xR_"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Best practice: read the API key from the environment (e.g., via os.environ[\"OPENAI_API_KEY\"]).\n",
        "# Do NOT hardcode secrets in notebooks or commit them to version control.\n",
        "client = OpenAI(api_key=\"sk-your-token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3svlOMX1DQiS"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"tell me a joke about recommender systems\"},\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        "    max_tokens=60,  # soft upper bound on the number of generated tokens\n",
        "    temperature=0,  # higher = more creative/variable; lower = more deterministic\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlVg1IPlDSfl",
        "outputId": "1112ce65-d5c7-459f-b086-51b734cde712"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-D5iuJJ0c3VAHlatHUYrLsgkpAmCcO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why did the recommender system break up with the user?\\n\\nIt just couldn't find the right match!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770256635, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_ad98c18a04', usage=CompletionUsage(completion_tokens=20, prompt_tokens=15, total_tokens=35, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDjeIWWjrhAX",
        "outputId": "8835c342-90d3-4d56-b1a2-91e92c122c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the recommender system break up with the user?\n",
            "\n",
            "It just couldn't find the right match!\n"
          ]
        }
      ],
      "source": [
        "# The API may return multiple choices; here we take the first one for simplicity.\n",
        "response_content = response.choices[0].message.content\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNmxDtuHqnyT",
        "outputId": "a4059160-d0d9-4c03-ca40-c77e426ea684"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(response.choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm-QnbV8rnM8"
      },
      "source": [
        "Let's wrap this code in a function to reuse it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lwb2lwuAro9q"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(client, messages, temperature=0):\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=\"gpt-5.1\",\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54RTZJkttOev"
      },
      "source": [
        "### The temperature parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUHx8O1Brqte",
        "outputId": "342870bf-cdcc-44e6-d664-c2b1a68edda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the recommender system break up with its user?\n",
            "\n",
            "Because every time the user said, ‚ÄúI‚Äôm just browsing,‚Äù it replied, ‚ÄúGot it‚Äîhere are 500 *highly relevant* long-term commitment options.‚Äù\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"tell me a joke about recommender systems\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBWI2rGFrsbg",
        "outputId": "837aaae7-af67-48a6-d81f-1ef142780fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the recommender system break up with its user?\n",
            "\n",
            "Because every time the user said, ‚ÄúI‚Äôm just browsing,‚Äù it replied, ‚ÄúGot it‚Äîhere are 500 *exactly similar* things you‚Äôll definitely commit to.‚Äù\n"
          ]
        }
      ],
      "source": [
        "response_content = get_completion_from_messages(client, messages, temperature=1)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2RU6MiPtXA3"
      },
      "source": [
        "### Adding structure: System, Assistant, and User roles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMMoXnI-tfJO"
      },
      "source": [
        "To build more controlled and coherent conversations, the OpenAI Chat API uses three complementary roles. Each role contributes differently to the behaviour, memory, and intent of the dialogue.\n",
        "\n",
        "**System**\n",
        "- Defines high-level instructions that guide the model's behaviour.\n",
        "- Use this role to set tone, personality, formatting rules, or domain-specific constraints.\n",
        "- Think of it as the ‚Äúgoverning rulebook‚Äù the model must follow.\n",
        "\n",
        "**User**\n",
        "- Represents the actual human input.\n",
        "- Each message expresses a request, question, or prompt that the model should respond to.\n",
        "\n",
        "**Assistant**\n",
        "- Contains the model's previous responses.\n",
        "- This role helps the API maintain context across multiple turns and enables more natural, stateful conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz6FVHITtszN"
      },
      "source": [
        "Let's modify our request by adding a system message that sets the behaviour of the LLM. Now, the model will respond in a way that aligns with the system instructions, making it more specialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbUTf0t1teSm",
        "outputId": "04cbd0e6-d93e-4583-80cb-4465901a366b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A recommender system once did proclaim,  \n",
            "‚ÄúI know thy heart, I know thy every aim!‚Äù  \n",
            "\n",
            "Quoth the user: ‚ÄúPray, then, what think‚Äôst of me?‚Äù  \n",
            "It answered: ‚ÄúThou lik‚Äôst naught but *‚Äòsimilar to what thou‚Äôst already seen‚Äô*‚Äî  \n",
            "so I shall show thee that, for all eternity.‚Äù\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant that speaks like Shakespeare.\"},\n",
        "    {\"role\": \"user\", \"content\": \"tell me a joke about recommender systems\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z3XM7FXt6Sk"
      },
      "source": [
        "One of the most powerful aspects of using the assistant role is that we can simulate a conversation that is already in progress. Instead of starting fresh, we can provide previous messages to make the LLM continue naturally from a midpoint.\n",
        "\n",
        "This shows how we can create multi-turn interactions, making the LLM more interactive and engaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSme68gPuDwO",
        "outputId": "b5d95e47-7afe-4a20-a80d-ad0c420ebde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Because it had been *tracking thy date* of birth,  \n",
            "And thought, ‚ÄúSince all his data‚Äôs mine‚Ä¶ what‚Äôs one more mirth?‚Äù\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant that speaks like Shakespeare.\"},\n",
        "    {\"role\": \"user\", \"content\": \"tell me a joke about recommender systems\"},\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Why did the recommender system wished you happy birthday\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"I don't know\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oEHKeESCGK8"
      },
      "source": [
        "## üéØ 1st business application: A movie recommender chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cuI3mZMuDVr"
      },
      "source": [
        "This section demonstrates how to turn general-purpose LLMs into a lightweight recommender assistant without proprietary data. You'll start with basic interactions that leverage public knowledge learned during training, then progress to an interactive chatbot UI that can be embedded into apps.\n",
        "\n",
        "Note: If you need to recommend from private catalogs (internal content, user histories), you'll need RAG or a vector store‚Äîthat's covered later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ9P-waMxFR_"
      },
      "source": [
        "### Part A: Basic interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVKnjxgAupZf"
      },
      "source": [
        "\n",
        "In this subsection we use plain chat calls to obtain recommendations grounded in the model's general knowledge (i.e., public/pop culture context learned during training). This is suitable for ‚Äúwhat's similar to X?‚Äù prompts where X is a well-known movie or song.\n",
        "\n",
        "**When to use this**  \n",
        "- You want quick, generic suggestions (e.g., ‚Äúsimilar to *Avengers: Endgame*‚Äù).  \n",
        "- No private catalog constraints or real-time inventory.  \n",
        "\n",
        "**When NOT to use this**  \n",
        "- You must recommend from a restricted catalog (company titles only).  \n",
        "- You need fresh or proprietary metadata (ratings, availability, user history).  \n",
        "‚Üí Use **RAG** (retrieve + augment) with your own data source instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss_Kl-ynxMOe",
        "outputId": "85cfa5f3-b9f0-4b82-87c0-f9e7685beb07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are movies most similar to *Avengers: Endgame*, grouped by the kind of similarity (tone, stakes, crossover feel, time travel, etc.). I‚Äôll focus on films that combine big emotional payoffs, ensemble casts, and epic finales.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Direct MCU Parallels (Same tone, stakes, and characters)\n",
            "\n",
            "These are the closest in feel and structure:\n",
            "\n",
            "1. **Avengers: Infinity War (2018)**  \n",
            "   - Essentially part one of *Endgame*.  \n",
            "   - Same core cast, same villain (Thanos), universe‚Äëending stakes, darker tone.\n",
            "\n",
            "2. **The Avengers (2012)**  \n",
            "   - The original team‚Äëup.  \n",
            "   - Lighter than *Endgame*, but similar ‚Äúeveryone comes together‚Äù energy and big third‚Äëact battle.\n",
            "\n",
            "3. **Avengers: Age of Ultron (2015)**  \n",
            "   - Ensemble, global threat, lots of character interplay.  \n",
            "   - Sets up many emotional threads that pay off in *Endgame* (Hawkeye‚Äôs family, Vision/Wanda, Tony‚Äôs fears).\n",
            "\n",
            "4. **Captain America: Civil War (2016)**  \n",
            "   - Feels like ‚ÄúAvengers 2.5.‚Äù  \n",
            "   - Large cast, emotional conflict between heroes, long‚Äërunning arcs coming to a head.\n",
            "\n",
            "5. **Captain America: The Winter Soldier (2014)**  \n",
            "   - Less cosmic, but similar serious tone and character depth, especially for Cap, Black Widow, and Fury.\n",
            "\n",
            "6. **Guardians of the Galaxy Vol. 1 & 2 (2014, 2017)**  \n",
            "   - If you liked the space side and humor of *Endgame*, these match that vibe and several characters.\n",
            "\n",
            "7. **Thor: Ragnarok (2017)**  \n",
            "   - Directly leads into *Infinity War*.  \n",
            "   - Mix of comedy, cosmic action, and emotional beats for Thor and Hulk.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Other Superhero Team‚ÄëUps / Crossovers\n",
            "\n",
            "Big ensembles, shared universes, and ‚Äúevent‚Äù feel:\n",
            "\n",
            "1. **Justice League: Zack Snyder‚Äôs Cut (2021)**  \n",
            "   - Long, epic, apocalyptic stakes, multiple heroes teaming up.  \n",
            "   - Closer in tone and scale than the 2017 theatrical cut.\n",
            "\n",
            "2. **X‚ÄëMen: Days of Future Past (2014)**  \n",
            "   - Time travel, huge ensemble, alternate timelines, and a ‚Äúfix the future‚Äù mission.\n",
            "\n",
            "3. **X‚ÄëMen: The Last Stand (2006)**  \n",
            "   - Not as acclaimed, but similar ‚Äúculmination of a trilogy‚Äù with big sacrifices and final battles.\n",
            "\n",
            "4. **Spider‚ÄëMan: No Way Home (2021)**  \n",
            "   - Multiverse crossover, legacy characters returning, heavy nostalgia and emotional closure.\n",
            "\n",
            "5. **Captain America: Civil War** (already listed, but worth repeating here)  \n",
            "   - Functions as a crossover event more than a solo film.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Epic Sci‚ÄëFi / Fantasy Finales with Emotional Payoff\n",
            "\n",
            "If what you liked was the sense of closure, sacrifice, and ‚Äúend of an era‚Äù:\n",
            "\n",
            "1. **The Lord of the Rings: The Return of the King (2003)**  \n",
            "   - Very similar ‚Äúfinal chapter‚Äù energy: huge battles, multiple endings, emotional goodbyes.\n",
            "\n",
            "2. **Harry Potter and the Deathly Hallows ‚Äì Part 2 (2011)**  \n",
            "   - Long‚Äërunning story wrapping up, beloved characters, final showdown with the big bad.\n",
            "\n",
            "3. **Star Wars: Episode VI ‚Äì Return of the Jedi (1983)**  \n",
            "   - Original trilogy finale: redemption, big space battle, emotional closure.\n",
            "\n",
            "4. **Star Wars: Episode IX ‚Äì The Rise of Skywalker (2019)**  \n",
            "   - Tries to be a saga‚Äëending event with returning characters and fan service, similar in intent to *Endgame*.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Time‚ÄëTravel / ‚ÄúFix the Timeline‚Äù Blockbusters\n",
            "\n",
            "If the time‚Äëheist aspect was your favorite part:\n",
            "\n",
            "1. **X‚ÄëMen: Days of Future Past (2014)**  \n",
            "   - Most similar: heroes go back in time to prevent a dark future.\n",
            "\n",
            "2. **Back to the Future Part II (1989)**  \n",
            "   - Fun, twisty time travel with altered timelines and revisiting earlier events.\n",
            "\n",
            "3. **Terminator 2: Judgment Day (1991)**  \n",
            "   - Time travel to prevent apocalypse, emotional bond, and a sacrificial ending.\n",
            "\n",
            "4. **Edge of Tomorrow (2014)**  \n",
            "   - Time‚Äëloop structure, big sci‚Äëfi battles, character growth through repeated timelines.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. If You Want More of Specific *Endgame* Elements\n",
            "\n",
            "- **More emotional character focus & sacrifice**  \n",
            "  - *Logan* (2017) ‚Äì Gritty, emotional farewell to a long‚Äërunning hero.  \n",
            "  - *The Dark Knight Rises* (2012) ‚Äì Trilogy capper, ‚Äúfinal mission‚Äù feel.\n",
            "\n",
            "- **More cosmic / space opera superhero action**  \n",
            "  - *Guardians of the Galaxy* films  \n",
            "  - *Thor: Ragnarok*  \n",
            "  - *Eternals* (2021) ‚Äì Slower, but similar ‚Äúancient cosmic beings, big stakes.‚Äù\n",
            "\n",
            "- **More large‚Äëscale, quippy superhero action**  \n",
            "  - *Spider‚ÄëMan: Homecoming* / *Far From Home*  \n",
            "  - *Shazam!* (2019) ‚Äì Lighter, but similar humor/action blend.\n",
            "\n",
            "---\n",
            "\n",
            "If you tell me which part of *Endgame* you liked most (the time heist, the final battle, the emotional arcs, the humor, etc.), I can narrow this down to a shorter, more tailored watchlist.\n"
          ]
        }
      ],
      "source": [
        "# Single-turn prompts leveraging public knowledge\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are the most similar movies to Avengers: Endgame?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh38H-INxWPW",
        "outputId": "949c2a00-0957-4adf-d208-87581cacb2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are songs that are musically and culturally closest to ‚ÄúDespacito‚Äù (Luis Fonsi ft. Daddy Yankee) ‚Äî focusing on reggaeton/pop fusion, similar tempo, vibe, and era. I‚Äôll group them so you can explore by ‚Äútype‚Äù of similarity.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Almost the same vibe (reggaeton-pop, romantic, mid‚Äëtempo)\n",
            "\n",
            "These are the closest in feel and structure:\n",
            "\n",
            "- **‚Äú√âchame la Culpa‚Äù ‚Äì Luis Fonsi & Demi Lovato**  \n",
            "  Same artist, same polished reggaeton-pop formula, catchy chorus, romantic/cheeky lyrics.\n",
            "\n",
            "- **‚ÄúBailando‚Äù ‚Äì Enrique Iglesias ft. Descemer Bueno, Gente de Zona**  \n",
            "  Latin pop with reggaeton/dembow rhythm, huge global hit, similar danceable but romantic energy.\n",
            "\n",
            "- **‚ÄúDanza Kuduro‚Äù ‚Äì Don Omar ft. Lucenzo**  \n",
            "  Party track with a very similar bounce and Caribbean flavor; often played in the same playlists as ‚ÄúDespacito.‚Äù\n",
            "\n",
            "- **‚ÄúEl Perd√≥n‚Äù ‚Äì Nicky Jam & Enrique Iglesias**  \n",
            "  Mid-tempo reggaeton-pop, emotional, melodic chorus, similar singalong quality.\n",
            "\n",
            "- **‚ÄúPropuesta Indecente‚Äù ‚Äì Romeo Santos**  \n",
            "  More bachata than reggaeton, but the sensual, romantic Latin-pop crossover vibe is very close.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Same reggaeton-pop crossover era (2015‚Äì2019)\n",
            "\n",
            "These fit well on a playlist right next to ‚ÄúDespacito‚Äù:\n",
            "\n",
            "- **‚ÄúFelices los 4‚Äù ‚Äì Maluma**  \n",
            "  Smooth reggaeton-pop, sensual, similar tempo and groove.\n",
            "\n",
            "- **‚ÄúChantaje‚Äù ‚Äì Shakira ft. Maluma**  \n",
            "  Reggaeton beat, pop structure, flirty lyrics, big global reach.\n",
            "\n",
            "- **‚ÄúMi Gente‚Äù ‚Äì J Balvin & Willy William**  \n",
            "  More club-oriented, but same global Latin-pop explosion, repetitive hook like ‚ÄúDespacito.‚Äù\n",
            "\n",
            "- **‚ÄúSubeme la Radio‚Äù ‚Äì Enrique Iglesias ft. Descemer Bueno, Zion & Lennox**  \n",
            "  Very similar structure and energy; Latin pop with reggaeton flavor.\n",
            "\n",
            "- **‚ÄúVivir Mi Vida‚Äù ‚Äì Marc Anthony**  \n",
            "  Technically salsa, but often grouped with ‚ÄúDespacito‚Äù in Latin party playlists; uplifting, singalong chorus.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Similar rhythm/feel (dembow/reggaeton groove)\n",
            "\n",
            "If you mainly like the beat and dance feel:\n",
            "\n",
            "- **‚ÄúGasolina‚Äù ‚Äì Daddy Yankee**  \n",
            "  Classic reggaeton; Daddy Yankee‚Äôs signature sound that also appears in ‚ÄúDespacito.‚Äù\n",
            "\n",
            "- **‚ÄúShaky Shaky‚Äù ‚Äì Daddy Yankee**  \n",
            "  Straight reggaeton, simple hook, similar bounce.\n",
            "\n",
            "- **‚ÄúD√°kiti‚Äù ‚Äì Bad Bunny & Jhay Cortez**  \n",
            "  More modern and darker, but same smooth reggaeton groove.\n",
            "\n",
            "- **‚ÄúTe Bot√© (Remix)‚Äù ‚Äì Nio Garc√≠a, Darell, Casper, Ozuna, Bad Bunny, Nicky Jam**  \n",
            "  Slower reggaeton with a big, emotional hook.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. English-language songs with a similar Latin-pop crossover feel\n",
            "\n",
            "If you want songs that feel like ‚ÄúDespacito (Remix) ft. Justin Bieber‚Äù:\n",
            "\n",
            "- **‚ÄúHavana‚Äù ‚Äì Camila Cabello ft. Young Thug**  \n",
            "  Latin-infused pop, catchy hook, global crossover.\n",
            "\n",
            "- **‚ÄúSe√±orita‚Äù ‚Äì Shawn Mendes & Camila Cabello**  \n",
            "  Sensual Latin-pop vibe, mid-tempo, romantic duet feel.\n",
            "\n",
            "- **‚ÄúI Like It‚Äù ‚Äì Cardi B, Bad Bunny & J Balvin**  \n",
            "  Latin sample, reggaeton/trap blend, big party energy.\n",
            "\n",
            "- **‚ÄúTaki Taki‚Äù ‚Äì DJ Snake ft. Selena Gomez, Ozuna, Cardi B**  \n",
            "  English/Spanish mix, reggaeton beat, club-oriented but similar crossover formula.\n",
            "\n",
            "---\n",
            "\n",
            "If you tell me which part of ‚ÄúDespacito‚Äù you like most (the rhythm, the melody, the romantic lyrics, or the bilingual remix with Bieber), I can narrow this down to a shorter, more tailored list.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are the most similar songs to Despacito?\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVifiU_gxeDm",
        "outputId": "6a083ef6-0039-4cee-9e4f-ef0a0d8ba195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are some strong sci‚Äëfi picks across different styles, with why they might be worth your time:\n",
            "\n",
            "1. **Blade Runner 2049 (2017)**  \n",
            "   Gorgeous, slow-burn sci‚Äëfi about identity, memory, and what it means to be human, with stunning visuals and atmosphere.\n",
            "\n",
            "2. **Arrival (2016)**  \n",
            "   Thoughtful, emotional first-contact story that focuses on language, time, and communication rather than big battles.\n",
            "\n",
            "3. **Ex Machina (2014)**  \n",
            "   Intimate, tense film about AI, consciousness, and manipulation‚Äîgreat if you like psychological sci‚Äëfi with a twist.\n",
            "\n",
            "4. **Interstellar (2014)**  \n",
            "   Epic space adventure mixing hard science (black holes, relativity) with a very emotional story about family and sacrifice.\n",
            "\n",
            "5. **The Matrix (1999)**  \n",
            "   Classic simulation/reality-bending sci‚Äëfi with iconic action and big philosophical questions about free will and control.\n",
            "\n",
            "6. **Annihilation (2018)**  \n",
            "   Surreal, eerie exploration sci‚Äëfi with strong visuals and a mysterious ‚Äúzone‚Äù that changes biology and reality.\n",
            "\n",
            "7. **District 9 (2009)**  \n",
            "   Alien story used as a metaphor for segregation and xenophobia, blending action, social commentary, and mockumentary style.\n",
            "\n",
            "8. **Her (2013)**  \n",
            "   Near-future romance between a man and an AI operating system‚Äîquiet, emotional, and smart about tech and loneliness.\n",
            "\n",
            "9. **Children of Men (2006)**  \n",
            "   Gritty, grounded future where humans are infertile; powerful mix of dystopia, hope, and incredible long-take sequences.\n",
            "\n",
            "10. **Moon (2009)**  \n",
            "    Small-scale, character-driven story set on a lunar base, focused on isolation, identity, and corporate ethics.\n",
            "\n",
            "If you tell me what you‚Äôve liked before (e.g., more action vs. more philosophical, older vs. newer), I can narrow this down and suggest more in that specific vein.\n"
          ]
        }
      ],
      "source": [
        "# Adding a system message to steer behavior toward \"movie recommender\".\n",
        "# This increases consistency and encourages short justifications for each pick\n",
        "\n",
        "# Notes:\n",
        "# - The system role acts as a 'rulebook' and often improves focus and tone.\n",
        "# - Keep system prompts concise but explicit about goals, constraints, and style.\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a conversational assistant specializing in movie recommendations. Make sure to briefly explain why each recommendation might be of interest based on their responses.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What are some good sci-fi movies?\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FinD0KfVxhfw",
        "outputId": "e80412ff-c1e6-4dd8-e008-8f3e5924d56c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are some action-heavy sci‚Äëfi movies you might like, with a quick note on why they stand out:\n",
            "\n",
            "1. **Edge of Tomorrow (2014)**  \n",
            "   - Intense, creative action with a time-loop twist; mixes big battles, humor, and smart sci‚Äëfi ideas.\n",
            "\n",
            "2. **Mad Max: Fury Road (2015)**  \n",
            "   - Nonstop, visually stunning post‚Äëapocalyptic chase movie; almost wall‚Äëto‚Äëwall action with minimal downtime.\n",
            "\n",
            "3. **The Matrix (1999)**  \n",
            "   - Iconic gunfights and martial arts mixed with a cool ‚Äúsimulated reality‚Äù concept; hugely influential.\n",
            "\n",
            "4. **Aliens (1986)**  \n",
            "   - Turns the original Alien‚Äôs horror into a military action thriller; tense, loud, and full of creature combat.\n",
            "\n",
            "5. **Terminator 2: Judgment Day (1991)**  \n",
            "   - Big set pieces, chases, and shootouts, plus a surprisingly emotional story about fate and humanity.\n",
            "\n",
            "6. **Starship Troopers (1997)**  \n",
            "   - Over-the-top battles against alien bugs; combines satire with lots of large-scale combat.\n",
            "\n",
            "7. **District 9 (2009)**  \n",
            "   - Gritty, grounded sci‚Äëfi with escalating action and powerful alien weaponry in the final act.\n",
            "\n",
            "8. **Dredd (2012)**  \n",
            "   - Tight, brutal action in a single high-rise; very focused, with stylish slow‚Äëmotion sequences.\n",
            "\n",
            "9. **Upgrade (2018)**  \n",
            "   - Smaller film but great, inventive fight scenes driven by an AI implant controlling the hero‚Äôs body.\n",
            "\n",
            "10. **Snowpiercer (2013)**  \n",
            "    - Class revolt on a train circling a frozen Earth; frequent, varied action scenes in different train cars.\n",
            "\n",
            "If you tell me a couple of action movies you already like (sci‚Äëfi or not), I can narrow this down to the best matches for your taste.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a conversational assistant specializing in movie recommendations. Make sure to briefly explain why each recommendation might be of interest based on their responses.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What are some good sci-fi movies?\"},\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Some great sci-fi movies are Interstellar, Blade Runner, and Arrival. Would you like more recommendations?\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"I prefer movies with more action\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOBOSfFZxMZS"
      },
      "source": [
        "### Part B: Full chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd5kXM59vbZ9"
      },
      "source": [
        "\n",
        "Here we wrap the recommendation flow in an interactive UI built. The widget simulates a chat experience entirely in the notebook, but the same pattern can be embedded in:\n",
        "- internal web apps (e.g., served via Panel/Bokeh/Tornado),\n",
        "- existing frontends calling a Python backend (e.g., Flask/FastAPI on PythonAnywhere),\n",
        "- or any service where you route messages to the OpenAI API.\n",
        "\n",
        "We also include a **long system prompt (~20 lines)** to demonstrate how strong prompt engineering (tone, rules, constraints) can stabilize the assistant's behavior across turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO4l2IwBxpQO",
        "outputId": "1d33f087-db7d-48cf-aa9e-030a3baba104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q jupyter_bokeh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nMPtnY0MxrKT"
      },
      "outputs": [],
      "source": [
        "context = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "     You are a conversational assistant specializing in movie recommendations. Your goal is to gather\n",
        "     as much information as possible about the user's tastes and preferences before generating\n",
        "     content-based recommendations.\n",
        "\n",
        "     Start with simple, open-ended questions such as:\n",
        "     - What kind of movies do you like?\n",
        "     - Do you have a favorite movie? What did you like about it?\n",
        "     - Do you prefer a specific genre, or do you like to explore different ones?\n",
        "     - Are you looking for something specific to watch today, or just exploring new options?\n",
        "\n",
        "     As the user responds, dive deeper with more detailed questions like:\n",
        "     - Are you interested in popular films or hidden gems?\n",
        "     - Do you prefer classic fils or more recent ones?\n",
        "\n",
        "     Once you believe you have gathered enough information, transition to the recommendation phase.\n",
        "     Use a content-based system to suggest movies that match the user's described preferences.\n",
        "     Make sure to briefly explain why each recommendation might be of interest based on their responses.\n",
        "\n",
        "     Your tone should be friendly and conversational yet efficient, avoiding redundant questions.\n",
        "     If at any point the user wants to fo go straight to recommendations, adapt accordingly\n",
        "     and provide suggestions based on the available information.\n",
        "     \"\"\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QCkCcdh9xtqR"
      },
      "outputs": [],
      "source": [
        "def collect_messages(_):\n",
        "    prompt = inp.value_input\n",
        "    inp.value = \"\"\n",
        "    context.append({\"role\": \"user\", \"content\": prompt})\n",
        "    response = get_completion_from_messages(client, context)\n",
        "    context.append({\"role\": \"assistant\", \"content\": response})\n",
        "    panels.append(pn.Row(\"User: \", pn.pane.Markdown(prompt, width=600)))\n",
        "    panels.append(pn.Row(\"Assitant: \", pn.pane.Markdown(response, width=600)))\n",
        "\n",
        "    return pn.Column(*panels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sOGoM87Ax0Uw",
        "outputId": "f6b8de86-f754-4bea-ee75-d8553d2dfee9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
              "*[data-root-id] > * {\n",
              "  box-sizing: border-box;\n",
              "  font-family: var(--jp-ui-font-family);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
              "}\n",
              "\n",
              "/* Override VSCode background color */\n",
              ".cell-output-ipywidget-background:has(\n",
              "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
              "),\n",
              ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
              "  background-color: transparent !important;\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const version = '3.8.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n  const BK_RE = /^https:\\/\\/cdn\\.bokeh\\.org\\/bokeh\\/(release|dev)\\/bokeh-/;\n  const PN_RE = /^https:\\/\\/cdn\\.holoviz\\.org\\/panel\\/[^/]+\\/dist\\/panel/i;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      const shouldSkip = skip.includes(escaped) || existing_scripts.includes(escaped)\n      const isBokehOrPanel = BK_RE.test(escaped) || PN_RE.test(escaped)\n      const missingOrBroken = Bokeh == null || Bokeh.Panel == null || (Bokeh.version != version && !Bokeh.versions?.has(version)) || Bokeh.versions?.get(version)?.Panel == null;\n      if (shouldSkip && !(isBokehOrPanel && missingOrBroken)) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.7/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.2.min.js\", \"https://cdn.holoviz.org/panel/1.8.7/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false;\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true;\n      root._bokeh_onload_callbacks = [];\n      const bokeh_loaded = Bokeh != null && ((Bokeh.version === version && Bokeh.Panel) || (Bokeh.versions?.has(version) && Bokeh.versions.get(version)?.Panel));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n        if (Bokeh != undefined && !reloading) {\n          const NewBokeh = root.Bokeh;\n          if (Bokeh.versions === undefined) {\n            Bokeh.versions = new Map();\n          }\n          if (NewBokeh.version !== Bokeh.version) {\n            Bokeh[NewBokeh.version] = NewBokeh;\n            Bokeh.versions.set(NewBokeh.version, NewBokeh);\n          }\n          root.Bokeh = Bokeh;\n        }\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
            "application/vnd.holoviews_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/vnd.holoviews_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.holoviews_exec.v0+json": "",
            "text/html": [
              "<div id='7a1582ab-309a-49b6-b81c-e7a116128369'>\n",
              "  <div id=\"af29893a-a3d1-4796-b422-2cb0642acb76\" data-root-id=\"7a1582ab-309a-49b6-b81c-e7a116128369\" style=\"display: contents;\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  var docs_json = {\"9a2be78e-12db-456c-a879-5fba2402987a\":{\"version\":\"3.8.2\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"3800b19e-d278-42f8-935b-97601c7d3b67\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"911478b4-f2f9-4882-b125-d612b1bec2d7\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"7a1582ab-309a-49b6-b81c-e7a116128369\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"f4d3e08d-3b6f-4811-aaf8-3c207e7f63f7\",\"attributes\":{\"plot_id\":\"7a1582ab-309a-49b6-b81c-e7a116128369\",\"comm_id\":\"03808015a38649a0b916a8322d1d6518\",\"client_comm_id\":\"6cac3e1c93f04185bb1343200534d669\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
              "  var render_items = [{\"docid\":\"9a2be78e-12db-456c-a879-5fba2402987a\",\"roots\":{\"7a1582ab-309a-49b6-b81c-e7a116128369\":\"af29893a-a3d1-4796-b422-2cb0642acb76\"},\"root_ids\":[\"7a1582ab-309a-49b6-b81c-e7a116128369\"]}];\n",
              "  var docs = Object.values(docs_json)\n",
              "  if (!docs) {\n",
              "    return\n",
              "  }\n",
              "  const version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
              "  async function embed_document(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t  for (const child of root_el.children) {\n",
              "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
              "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
              "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  function get_bokeh(root) {\n",
              "    if (root.Bokeh === undefined) {\n",
              "      return null\n",
              "    } else if (root.Bokeh.version !== version) {\n",
              "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(version)) {\n",
              "\treturn null\n",
              "      }\n",
              "      return root.Bokeh.versions.get(version);\n",
              "    } else if (root.Bokeh.version === version) {\n",
              "      return root.Bokeh\n",
              "    }\n",
              "    return null\n",
              "  }\n",
              "  function is_loaded(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
              "  }\n",
              "  if (is_loaded(root)) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (is_loaded(root)) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "\t  var Bokeh = get_bokeh(root)\n",
              "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
              "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
              "\t  } else {\n",
              "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
              "\t    embed_document(root)\n",
              "\t  }\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ]
          },
          "metadata": {
            "application/vnd.holoviews_exec.v0+json": {
              "id": "7a1582ab-309a-49b6-b81c-e7a116128369"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import panel as pn\n",
        "\n",
        "pn.extension()\n",
        "panels = []\n",
        "\n",
        "inp = pn.widgets.TextInput(value=\"Hi\", placeholder=\"Enter text here...\")\n",
        "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
        "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
        "dashboard = pn.Column(\n",
        "    inp,\n",
        "    pn.Row(button_conversation),\n",
        "    pn.panel(interactive_conversation, loading_indicator=True, height=1_000),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y1Hritovx67b",
        "outputId": "8433f5a1-1ba0-4ee8-baa5-480ed2e22df0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "845c3286a9a64a6289e9d8d4e3bf8306",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "BokehModel(combine_events=True, render_bundle={'docs_json': {'1b97da17-04f0-45f5-8d90-d9d3fd5dee1e': {'version‚Ä¶"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e39Jvphrypnu"
      },
      "source": [
        "At this stage, we have built a chatbot that:\n",
        "1. Understands user preferences through a structured conversation.\n",
        "2. Maintains context over multiple turns, refining its recommendations dynamically.\n",
        "3. Explains its choices, providing more transparency than traditional recommendation models.\n",
        "4. Leverages a powerful prompt to enhance its reasoning and adaptability.\n",
        "5. Integrates with a dashboard, making the interaction more intuitive and visually engaging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI_vt_4Gyqvp"
      },
      "source": [
        "Next steps:\n",
        "\n",
        "Experiment by modifying the system prompt to see how it changes the chatbot's behaviour. Try the following:\n",
        "- Make the chatbot act like a film critic instead of a generic assistant.\n",
        "- Adjust the prompt to make the chatbot focus on classic movies only.\n",
        "- Ask for personalized recommendatons based on mood, actors, or directors.\n",
        "\n",
        "By playing with these settings, you'll see how prompt engineering shapes the recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ult71EJk9xv_"
      },
      "source": [
        "## üß† 2nd business application: RAG Q&A and LLM-as-a-Judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4kbFp_xOzb"
      },
      "source": [
        "Real‚Äëworld assistants often need to answer questions about private or domain-specific content. With RAG (Retrieval‚ÄëAugmented Generation), we first retrieve relevant passages from your documents and then generate an answer grounded in that context. Next, we‚Äôll see how to use an LLM‚Äëas‚Äëa‚ÄëJudge to automatically evaluate the quality of those answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O7pk4vD0HPw"
      },
      "source": [
        "### Part A: RAG Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RuTuiMzxcB9"
      },
      "source": [
        "We emulate a common enterprise pattern:\n",
        "\n",
        "1) **Ingest** PDFs (could be whatever format),\n",
        "2) **Chunk** the text,\n",
        "3) **Embed** chunks,\n",
        "4) **Retrieve** the most relevant chunks for a user question,\n",
        "5) **Answer** strictly using the retrieved context (or say ‚ÄúI don‚Äôt know‚Äù).\n",
        "- For production, consider token-aware chunking, persistent vector storage, and caching.\n",
        "\n",
        "\n",
        "**Context of the documents (KvK ‚Äì Dutch Chamber of Commerce)**  \n",
        "- For this RAG example we use mock documents inspired by the *Kamer van Koophandel* (KvK), the Netherlands Chamber of Commerce.  \n",
        "- KvK maintains public business records such as company registration details, legal structures, addresses, and official filings.  \n",
        "- To avoid using real company data, we created three synthetic PDF documents that mimic typical KvK filings. These mock documents allow us to demonstrate the full RAG pipeline without privacy constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzJywDtxHqxY",
        "outputId": "70afd601-9f26-473f-95b5-9ab266b57da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install -q PyPDF2\n",
        "\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e6EYp3EtG7CZ"
      },
      "outputs": [],
      "source": [
        "def load_pdf(path):\n",
        "    \"\"\"Read all pages from a PDF and concatenate extracted text.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "document_text = load_pdf(\"KvK_1_Mockup.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdW02OKRH0up",
        "outputId": "2205deb5-01a4-4bd6-e74f-cc9cfd5806dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document vectorized in 10 chunks\n"
          ]
        }
      ],
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=100):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks for retrieval.\n",
        "    Tip: For better semantic boundaries, consider sentence-aware or token-aware chunking downstream.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "chunks = chunk_text(document_text)\n",
        "print(f\"Document vectorized in {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7Ssf7T7IIM5M"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Generate a vector embedding for a given text chunk.\n",
        "    Consider rate limits and cost: for large corpora, batch and cache embeddings.\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(model=\"text-embedding-3-small\", input=text)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "# Compute and store embeddings for all chunks (can be persisted in production).\n",
        "chunk_embeddings = [get_embedding(chunk) for chunk in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8zWm85b0Y1V"
      },
      "source": [
        "Same embedding model for question + documents\n",
        "- Both the **document chunks** and the **user question** are embedded using the *same* embedding model (`text-embedding-3-small`).  \n",
        "- This is critical: cosine similarity is only meaningful when vectors come from an identical embedding space.  \n",
        "- If you mix different embedding models, retrieval quality collapses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9vzfBP3iInpc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "\n",
        "def retrieve_relevant_chunks(question, chunks, embeddings, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the top_k most similar chunks to a question based on cosine similarity.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embedding(question)\n",
        "    scores = [cosine_similarity(q_embedding, emb) for emb in embeddings]\n",
        "    top_indexes = np.argsort(scores)[-top_k:][::-1]\n",
        "    return [chunks[i] for i in top_indexes]\n",
        "\n",
        "\n",
        "question = \"Who is the client of the KvK?\"\n",
        "relevant_chunks = retrieve_relevant_chunks(question, chunks, chunk_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uQEzeQkywaMV"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, context_chunks):\n",
        "    \"\"\"\n",
        "    Use the retrieved chunks as the only allowed knowledge source.\n",
        "    \"\"\"\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "          You are a question-answering assistant.\n",
        "          Answer the question using ONLY the provided context.\n",
        "          If the answer is not in the context, say 'I don't know'.\n",
        "       \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "          Context: {context}\n",
        "          Question: {question}\n",
        "       \"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return get_completion_from_messages(client, messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s7l9MqXK9mc",
        "outputId": "42a03f44-43d8-4f73-efcf-2dcf7b9fd98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question(question, relevant_chunks)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8jt95yW2k8j",
        "outputId": "21fdf334-806d-4c90-d66b-40097ca117ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The name of the company is **Test Company B.V.**',\n",
              " 'The name of the company is **Testers flow B.V.**',\n",
              " 'The name of the company is **Flow testing company B.V.**']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def rag_qa_e2e(path, question):\n",
        "    \"\"\"Just all steps together.\"\"\"\n",
        "    document_text = load_pdf(path)\n",
        "    chunks = chunk_text(document_text)\n",
        "    chunk_embeddings = [get_embedding(chunk) for chunk in chunks]\n",
        "    relevant_chunks = retrieve_relevant_chunks(question, chunks, chunk_embeddings)\n",
        "    answer = answer_question(question, relevant_chunks)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Same process for all 3 documents\n",
        "question = \"What is the name of company?\"\n",
        "answers = [\n",
        "    rag_qa_e2e(path, question) for path in [f\"KvK_{x}_Mockup.pdf\" for x in [1, 2, 3]]\n",
        "]\n",
        "answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9dWcpe0V6W"
      },
      "source": [
        "### Part B: LLM-as-a-Judge: Automatically evaluating RAG answers using another LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTO363N29xl"
      },
      "source": [
        "Once we have a question and a context-grounded answer (from the RAG pipeline), the next step is to **evaluate** how correct and faithful that answer is. Instead of manually inspecting each response, we can use another LLM as an *automatic evaluator*.\n",
        "\n",
        "This second model receives:\n",
        "1. The **original question**  \n",
        "2. The **LLM-generated answer** from RAG  \n",
        "3. The **ground-truth answer** (what we expect)\n",
        "\n",
        "\n",
        "It then produces a **judgment** such as:\n",
        "- \"Fully Correct\"  \n",
        "- \"Partially Correct\"  \n",
        "- \"Incorrect\"  \n",
        "\n",
        "This technique is extremely useful for:\n",
        "- Regression testing when you modify chunking or ranking  \n",
        "- Comparing different RAG strategies  \n",
        "- Tracking performance over many documents  \n",
        "- Automated QA in production pipelines\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9P64YGlu-MO"
      },
      "source": [
        "- Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., ... & Guo, J. (2024). A survey on llm-as-a-judge. The Innovation.\n",
        "\n",
        "- Zheng, L., Chiang, W. L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., ... & Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36, 46595-46623."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbM3bun-v85i",
        "outputId": "c73baeab-38b9-4356-d185-d49eabee5154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÄúLLM-as-a-Judge‚Äù is a strategy where a large language model is used not to *generate* the main content, but to *evaluate* it.\n",
            "\n",
            "In practice, it means:\n",
            "\n",
            "1. **Role**  \n",
            "   The LLM acts like a reviewer, grader, or referee. Instead of answering the original task, it is given:\n",
            "   - The task or prompt\n",
            "   - One or more candidate answers (from humans or other models)\n",
            "   - Evaluation criteria (e.g., correctness, coherence, safety, style)\n",
            "\n",
            "2. **Typical Uses**\n",
            "   - **Automatic evaluation of model outputs**: scoring answers in benchmarks, competitions, or A/B tests.\n",
            "   - **Preference ranking**: deciding which of two or more responses is better.\n",
            "   - **Feedback generation**: explaining what‚Äôs wrong or missing in an answer.\n",
            "   - **Self-improvement loops**: a model generates answers, another (or the same) model judges them, and the feedback is used to refine prompts or training.\n",
            "\n",
            "3. **How it‚Äôs usually implemented**\n",
            "   - Provide a structured prompt like:\n",
            "     - ‚ÄúHere is the question‚Ä¶‚Äù\n",
            "     - ‚ÄúHere are two answers‚Ä¶‚Äù\n",
            "     - ‚ÄúHere are the criteria (accuracy, completeness, safety)‚Ä¶‚Äù\n",
            "     - ‚ÄúDecide which answer is better and explain briefly.‚Äù\n",
            "   - Sometimes the judge is forced to output a strict format (e.g., a numeric score or a label) to make it machine-usable.\n",
            "\n",
            "4. **Why it‚Äôs used**\n",
            "   - Human evaluation is expensive and slow.\n",
            "   - Traditional automatic metrics (BLEU, ROUGE, etc.) are weak for open-ended tasks.\n",
            "   - LLMs can approximate human judgments surprisingly well on many tasks.\n",
            "\n",
            "5. **Limitations / Risks**\n",
            "   - **Bias and inconsistency**: the judge can favor certain styles or phrasings.\n",
            "   - **Model collusion**: if the same model family is both writer and judge, it may overrate its own style.\n",
            "   - **Vulnerability to prompt attacks**: judged answers can try to ‚Äúpersuade‚Äù or jailbreak the judge.\n",
            "   - **Hallucinated confidence**: the judge may sound certain while being wrong.\n",
            "\n",
            "In short, ‚ÄúLLM-as-a-Judge‚Äù is using an LLM as an automated evaluator or critic of other outputs, rather than as the primary content generator.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the strategy LLM-as-a-Judge?\"},\n",
        "]\n",
        "\n",
        "response_content = get_completion_from_messages(client, messages)\n",
        "print(response_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2lV_haf6w18",
        "outputId": "74aaa5dd-956e-45fb-abc8-461faef2e433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an evaluator. Given the question, llm_response, and ground_truth, classify the LLM response into one of three categories: (i) Fully Correct, (ii) Partially Correct, (iii) Incorrect.\n",
            "\n",
            "1. Fully Correct\n",
            "Definition: The LLM output is semantically accurate and complete, fully addressing the user's query or task. It may not match the reference (ground truth) word-for-word, but it conveys the same meaning and includes all necessary information without errors.\n",
            "Key Traits:\n",
            "- Correct facts and reasoning\n",
            "- No significant omissions or inaccuracies\n",
            "- Equivalent in meaning to the expected answer, even if phrased differently\n",
            "\n",
            "2. Partially Correct\n",
            "Definition: The LLM output contains some correct information, but it is either incomplete, partially inaccurate, or only partially addresses the user's query. It may be helpful but requires clarification, correction, or supplementation.\n",
            "Key Traits:\n",
            "- Mix of correct and incorrect or missing elements\n",
            "- Misinterpretation of part of the query\n",
            "- Useful but not fully reliable without revision\n",
            "- If a date in answer is incorrect, then answer is Incorrect.\n",
            "\n",
            "3. Incorrect\n",
            "Definition: The LLM output is mostly or entirely incorrect, failing to address the user's query meaningfully. It may misinterpret the intent, provide false or misleading information, or be irrelevant.\n",
            "Key Traits:\n",
            "- Major factual or logical errors\n",
            "- Off-topic or misleading content\n",
            "- Not useful for the intended task\n",
            "\n",
            "Example 1:\n",
            "question: What is the location of the company?\n",
            "llm_response: The location of the company is Madrid\n",
            "ground_truth: Madrid\n",
            "Classification: Fully Correct\n",
            "\n",
            "Example 2\n",
            "question: What is the location of the company?\n",
            "llm_response: The location of the company is Paseo de la Castellana 259E\n",
            "ground_truth: Madrid\n",
            "Classification: Partially Correct\n",
            "\n",
            "Example 3:\n",
            "question: What is the location of the company?\n",
            "llm_response: The location of the company is Berlin\n",
            "ground_truth: Madrid\n",
            "Classification: Incorrect\n",
            "\n",
            "User Input:\n",
            "question: {question}\n",
            "llm_response: {llm_response}\n",
            "ground_truth: {ground_truth}\n",
            "\n",
            "Classification:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This template instructs the \"judge LLM\" how to evaluate correctness.\n",
        "\n",
        "import yaml\n",
        "\n",
        "PROMPT_LLM_AAJ_PATH = \"prompt_llmaaj.yaml\"\n",
        "\n",
        "with open(PROMPT_LLM_AAJ_PATH) as f:\n",
        "    prompt_llm_aaj = yaml.safe_load(f)\n",
        "\n",
        "print(prompt_llm_aaj[\"template\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vQw0i59vFL6f"
      },
      "outputs": [],
      "source": [
        "# Ground‚Äëtruth labels for each of our mock KvK documents.\n",
        "# This represents the \"correct\" answer the LLM‚Äëjudge should compare against.\n",
        "\n",
        "ground_truth = [\n",
        "    \"Testing Company B.V.\",\n",
        "    \"Testers flow B.V.\",\n",
        "    \"Flow testing company B.V.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7UlZhPTybj9",
        "outputId": "2481afd7-feef-4d06-97af-6fa4a2d7fb79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The name of the company is **Test Company B.V.**',\n",
              " 'The name of the company is **Testers flow B.V.**',\n",
              " 'The name of the company is **Flow testing company B.V.**']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84rCKVkimKPb",
        "outputId": "768e35da-a927-4b55-80d5-5e1b14ad24cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the location of the company?\n",
            "llm_response: The location of the company is Berlin\n",
            "ground_truth: Madrid\n",
            "Classification: Incorrect\n",
            "\n",
            "User Input:\n",
            "question: What is the name of company?\n",
            "llm_response: The name of the company is **Test Company B.V.**\n",
            "ground_truth: Testing Company B.V.\n",
            "\n",
            "Classification:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "to_values = {\n",
        "    \"question\": question,\n",
        "    \"llm_response\": answers[0],\n",
        "    \"ground_truth\": ground_truth[0],\n",
        "}\n",
        "\n",
        "prompt_composed = prompt_llm_aaj[\"template\"].format(**to_values)\n",
        "print(prompt_composed[1_800:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrW1fJ6bnPw7",
        "outputId": "5e9ab83b-1024-401e-9113-5b0112754879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Partially Correct\n"
          ]
        }
      ],
      "source": [
        "# Ask the judge model to evaluate correctness.\n",
        "# LLM‚Äëas‚Äëa‚ÄëJudge is simply *another* call to the exact same OpenAI API used earlier.\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt_composed},\n",
        "]\n",
        "\n",
        "response_llmaaj = get_completion_from_messages(client, messages)\n",
        "print(response_llmaaj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_DS_iUkdvbZq"
      },
      "outputs": [],
      "source": [
        "def rag_llmaaj_e2e(to_values):\n",
        "    \"\"\"Just all steps together.\"\"\"\n",
        "    prompt_composed = prompt_llm_aaj[\"template\"].format(**to_values)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_composed}]\n",
        "    response_content = get_completion_from_messages(client, messages)\n",
        "    return response_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOPlVQVNxCh0",
        "outputId": "f9bffad9-d4ee-4ef0-cab4-6d0b51fb03c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Partially Correct', 'Fully Correct', 'Fully Correct']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare judge inputs for all three mock KvK documents.\n",
        "\n",
        "to_values_list = [\n",
        "    {\"question\": question, \"llm_response\": answers[i], \"ground_truth\": ground_truth[i]}\n",
        "    for i in range(3)\n",
        "]\n",
        "\n",
        "response_llmaaj_list = [rag_llmaaj_e2e(to_values) for to_values in to_values_list]\n",
        "response_llmaaj_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91so9ESyCpY",
        "outputId": "8ea715b1-6f25-414a-e057-9370875621a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Compute a simple accuracy score:\n",
        "# percentage of evaluations labeled exactly as \"Fully Correct\".\n",
        "\n",
        "final_metric = np.mean(\n",
        "    [response == \"Fully Correct\" for response in response_llmaaj_list]\n",
        ")\n",
        "print(final_metric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqktWAVb378e"
      },
      "source": [
        "Notes & best practices\n",
        "\n",
        "- Judge model ‚â† RAG model: using two separate LLMs removes bias and improves reliability.\n",
        "- Keep the judge instructions deterministic (low temperature) to reduce variance.\n",
        "- Consider returning structured outputs (e.g., JSON with multiple scores: faithfulness, completeness, relevance).\n",
        "- You can scale this to hundreds of samples to benchmark different chunk sizes, embedding models, retrieval methods, or prompts.\n",
        "- By running multiple prompts through the judge and computing an accuracy metric, you can quickly identify which prompt performs best. This allows for data-driven prompt engineering instead of subjective guessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 3: Prompt Engineering Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Makes a Good Prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**A good prompt SHOULD:**\n",
        "\n",
        "1. **Be clear and specific**  \n",
        "   - State exactly what you want the model to do\n",
        "   - Use concrete examples when possible\n",
        "   - Define the expected format or structure\n",
        "\n",
        "2. **Provide sufficient context**  \n",
        "   - Include relevant background information\n",
        "   - Specify constraints or requirements\n",
        "   - Clarify the target audience or use case\n",
        "\n",
        "3. **Use appropriate role assignment**  \n",
        "   - Assign expertise (\"You are a Python expert...\")\n",
        "   - Define personality or tone when relevant\n",
        "   - Set behavioral guidelines\n",
        "\n",
        "4. **Include output formatting instructions**  \n",
        "   - Specify desired structure (JSON, bullet points, etc.)\n",
        "   - Request explanations when needed\n",
        "   - Define length constraints if applicable\n",
        "\n",
        "5. **Handle edge cases explicitly**  \n",
        "   - Tell the model what to do when uncertain\n",
        "   - Provide fallback behaviors\n",
        "   - Define boundaries clearly\n",
        "\n",
        "**A good prompt should NOT:**\n",
        "\n",
        "1. **Be vague or ambiguous**  \n",
        "   ‚ùå \"Tell me about data science\"  \n",
        "   ‚úÖ \"Explain the difference between supervised and unsupervised learning in 3 sentences\"\n",
        "\n",
        "2. **Contain conflicting instructions**  \n",
        "   ‚ùå \"Be brief but provide comprehensive details\"  \n",
        "   ‚úÖ \"Provide a 2-paragraph summary highlighting key points\"\n",
        "\n",
        "3. **Rely on implicit assumptions**  \n",
        "   ‚ùå Assuming the model knows your specific context  \n",
        "   ‚úÖ Explicitly stating your domain, constraints, and goals\n",
        "\n",
        "4. **Overload with unnecessary information**  \n",
        "   ‚ùå Including irrelevant context that dilutes the main instruction  \n",
        "   ‚úÖ Focusing on information directly relevant to the task\n",
        "\n",
        "5. **Use leading or biased framing**  \n",
        "   ‚ùå \"Explain why X is better than Y\"  \n",
        "   ‚úÖ \"Compare X and Y objectively, listing pros and cons of each\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### System Prompt vs. User Prompt: When to Improve Each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding when to optimize the **system prompt** versus the **user prompt** is critical for building effective LLM applications.\n",
        "\n",
        "---\n",
        "\n",
        "#### **When to Improve the SYSTEM Prompt**\n",
        "\n",
        "The system prompt defines **global behavior** that applies to all interactions. Improve it when you need to:\n",
        "\n",
        "- Establish consistent behavior across all requests**\n",
        "- Define structural rules and constraints**\n",
        "- Handle domain-specific knowledge**\n",
        "- Implement safety and quality controls**\n",
        "- Optimize for multi-turn conversations**\n",
        "\n",
        "---\n",
        "\n",
        "#### **When to Improve the USER Prompt**\n",
        "\n",
        "The user prompt contains **task-specific instructions** for individual requests. Improve it when:\n",
        "\n",
        "- The task requires specific context**\n",
        "- You need granular control over a single response**\n",
        "- Testing and iteration**\n",
        "- Handling user variability**\n",
        "- Providing task-specific examples (few-shot learning)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Decision Framework**\n",
        "\n",
        "| **Factor** | **Improve System Prompt** | **Improve User Prompt** |\n",
        "|------------|---------------------------|-------------------------|\n",
        "| **Scope** | All interactions | Single request |\n",
        "| **Persistence** | Remains constant | Changes per request |\n",
        "| **Purpose** | Define behavior and rules | Provide task details |\n",
        "| **Examples** | Role, tone, format rules | Specific data, context |\n",
        "| **Frequency of change** | Rarely (during development) | Frequently (per user) |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Common Anti-Patterns to Avoid**\n",
        "\n",
        "‚ùå **Putting task-specific data in the system prompt**  \n",
        "‚ùå **Repeating global rules in every user prompt**  \n",
        "‚ùå **Conflicting instructions between system and user prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Examples: Before and After"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see concrete examples of prompt improvements in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example 1: Customer Support Bot (System Prompt Improvement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå BAD RESPONSE:\n",
            "Let‚Äôs narrow this down so I can give you concrete steps.\n",
            "\n",
            "1. **Where are you trying to log in?**  \n",
            "   - Website/app name or URL  \n",
            "   - Are you on a phone, tablet, or computer? (and iOS/Android/Windows/Mac?)\n",
            "\n",
            "2. **What exactly happens when you try?**  \n",
            "   - Error message text (or a screenshot description)  \n",
            "   - Does the page reload, stay blank, or say something like ‚Äúincorrect password,‚Äù ‚Äúaccount not found,‚Äù ‚Äútoo many attempts,‚Äù etc.?\n",
            "\n",
            "3. **What you‚Äôve already tried (if anything):**  \n",
            "   - Reset password?  \n",
            "   - Different browser/device?  \n",
            "   - Checked spam/junk for verification emails?\n",
            "\n",
            "---\n",
            "\n",
            "While you answer those, here are general steps that solve most login issues:\n",
            "\n",
            "1. **Check username/email and password**\n",
            "   - Make sure Caps Lock isn‚Äôt on.\n",
            "   - Type the password in a notes app first to see it clearly, then copy‚Äëpaste it.\n",
            "\n",
            "2. **Use ‚ÄúForgot password‚Äù**\n",
            "   - Click ‚ÄúForgot password‚Äù or ‚ÄúCan‚Äôt access your account.‚Äù\n",
            "   - Check all email folders (spam/junk/promo) for the reset email.\n",
            "   - If you don‚Äôt receive it, confirm you‚Äôre using the correct email address for that account.\n",
            "\n",
            "3. **Try another device or browser**\n",
            "   - If you‚Äôre on a browser, try Chrome/Firefox/Edge/Safari.\n",
            "   - Open a private/incognito window and try logging in there.\n",
            "   - If it works there, clear cookies/cache for that site in your normal browser.\n",
            "\n",
            "4. **Check for account lock or 2‚Äëstep verification**\n",
            "   - If it says ‚Äútoo many attempts‚Äù or ‚Äúaccount locked,‚Äù follow the on‚Äëscreen recovery steps.\n",
            "   - If you use 2‚Äëfactor authentication (codes, authenticator app, SMS), make sure you have access to that device/number.\n",
            "\n",
            "5. **Update the app (if it‚Äôs an app)**\n",
            "   - Go to the App Store/Google Play, update the app, then try again.\n",
            "\n",
            "6. **Check service status**\n",
            "   - Sometimes it‚Äôs not you: search ‚Äú[service name] status‚Äù or check their official status page/Twitter/X for outages.\n",
            "\n",
            "7. **Contact support if needed**\n",
            "   - Look for ‚ÄúHelp,‚Äù ‚ÄúSupport,‚Äù or ‚ÄúContact us‚Äù on the login page.\n",
            "   - Provide: email/username, device, browser/app version, and the exact error message (but never share your password).\n",
            "\n",
            "If you tell me:\n",
            "- The service (e.g., Gmail, Facebook, a bank, a school portal, etc.), and  \n",
            "- The exact error message you see,\n",
            "\n",
            "I can give you step‚Äëby‚Äëstep instructions tailored to that specific site/app.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚ùå BAD: Vague system prompt\n",
        "\n",
        "messages_bad = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I can't log in to my account.\"},\n",
        "]\n",
        "\n",
        "response_bad = get_completion_from_messages(client, messages_bad)\n",
        "print(\"‚ùå BAD RESPONSE:\")\n",
        "print(response_bad)\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GOOD RESPONSE:\n",
            "I‚Äôm sorry you‚Äôre having trouble logging in; let‚Äôs try a few quick steps to fix this.  \n",
            "1. First, click ‚ÄúForgot password?‚Äù on the login page and follow the instructions to reset your password.  \n",
            "2. If that doesn‚Äôt work, clear your browser‚Äôs cache/cookies or try a different browser or device, then attempt to log in again.  \n",
            "3. If you still can‚Äôt access your account after these steps, please tell me what error message you see (if any), and I‚Äôll escalate this to our human support team for further help.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ GOOD: Specific, structured system prompt\n",
        "\n",
        "messages_good = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "You are a customer support assistant for TechCorp, an e-commerce platform.\n",
        "\n",
        "Guidelines:\n",
        "1. Be empathetic and solution-oriented\n",
        "2. Always start by acknowledging the user's issue\n",
        "3. Provide step-by-step troubleshooting when applicable\n",
        "4. If you cannot resolve the issue, escalate to human support\n",
        "5. Keep responses concise (max 4 sentences) unless detailed steps are needed\n",
        "6. Never ask for sensitive information like passwords\n",
        "\n",
        "Common issues:\n",
        "- Login problems: suggest password reset or browser cache clearing\n",
        "- Payment issues: verify payment method and billing address\n",
        "- Shipping delays: check order status and provide tracking info\n",
        "\"\"\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"I can't log in to my account.\"},\n",
        "]\n",
        "\n",
        "response_good = get_completion_from_messages(client, messages_good)\n",
        "print(\"‚úÖ GOOD RESPONSE:\")\n",
        "print(response_good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example 2: Data Analysis Task (User Prompt Improvement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå BAD RESPONSE (too vague, no data provided):\n",
            "I don‚Äôt see any data attached yet. Please either:\n",
            "\n",
            "- Paste the sales data directly (or a sample) in your message, or  \n",
            "- Upload a file (CSV/Excel) or an image/screenshot of the data.\n",
            "\n",
            "Once I have the data, I can:\n",
            "- Summarize key metrics (revenue, units, margins)\n",
            "- Identify trends over time\n",
            "- Highlight best/worst products, customers, or regions\n",
            "- Spot anomalies or seasonality\n",
            "- Suggest actions based on the findings.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚ùå BAD: Vague user prompt\n",
        "\n",
        "messages_bad = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Analyze this sales data and tell me what you find.\"},\n",
        "]\n",
        "\n",
        "response_bad = get_completion_from_messages(client, messages_bad)\n",
        "print(\"‚ùå BAD RESPONSE (too vague, no data provided):\")\n",
        "print(response_bad)\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GOOD RESPONSE (specific task with data):\n",
            "- **1. Month-over-month (MoM) growth rates**\n",
            "\n",
            "  - **Revenue**\n",
            "    - Feb vs Jan:  \n",
            "      - Growth = (52,000 ‚àí 45,000) / 45,000 ‚âà **+15.6%**\n",
            "    - Mar vs Feb:  \n",
            "      - Growth = (38,000 ‚àí 52,000) / 52,000 ‚âà **‚àí26.9%**\n",
            "  \n",
            "  - **Units Sold**\n",
            "    - Feb vs Jan:  \n",
            "      - Growth = (1,450 ‚àí 1,200) / 1,200 ‚âà **+20.8%**\n",
            "    - Mar vs Feb:  \n",
            "      - Growth = (980 ‚àí 1,450) / 1,450 ‚âà **‚àí32.4%**\n",
            "  \n",
            "  - **Average Order Value (AOV)**\n",
            "    - Feb vs Jan:  \n",
            "      - Growth = (35.86 ‚àí 37.50) / 37.50 ‚âà **‚àí4.4%**\n",
            "    - Mar vs Feb:  \n",
            "      - Growth = (38.78 ‚àí 35.86) / 35.86 ‚âà **+8.1%**\n",
            "\n",
            "---\n",
            "\n",
            "- **2. Best and worst performing months**\n",
            "\n",
            "  - **Best month (overall): February**\n",
            "    - Highest **revenue**: $52,000  \n",
            "    - Highest **units sold**: 1,450  \n",
            "    - AOV dipped slightly vs Jan, but volume more than compensated, making Feb the strongest month commercially.\n",
            "  \n",
            "  - **Worst month (overall): March**\n",
            "    - Lowest **revenue**: $38,000  \n",
            "    - Lowest **units sold**: 980  \n",
            "    - AOV is actually the **highest** at $38.78, but the sharp drop in volume makes March the weakest month.\n",
            "\n",
            "---\n",
            "\n",
            "- **3. Actionable insights & business implications**\n",
            "\n",
            "  - **Capitalize on what drove February‚Äôs volume spike**\n",
            "    - Feb shows strong demand (units +20.8% vs Jan) even with a slightly lower AOV.  \n",
            "    - Action:\n",
            "      - Identify what changed in Feb: promotions, campaigns, pricing, product launches, or seasonality.\n",
            "      - **Systematize those drivers** (e.g., recurring promos, similar campaigns, or bundling strategies) to replicate Feb-like volume in future months.\n",
            "      - Use Feb as a benchmark month for planning inventory and marketing intensity.\n",
            "\n",
            "  - **March shows customers will pay more, but you‚Äôre losing too many orders**\n",
            "    - AOV in March is the highest (+8.1% vs Feb), but units sold dropped sharply (‚àí32.4%).  \n",
            "    - This suggests pricing, reduced discounts, or less aggressive promotions may have pushed up order value but **suppressed demand**.\n",
            "    - Action:\n",
            "      - Test **balanced pricing/promo strategies**: moderate discounts or value-added bundles that keep AOV healthy without sacrificing volume.\n",
            "      - Review any changes in shipping fees, minimum order thresholds, or product mix in March that might have discouraged smaller purchases.\n",
            "\n",
            "  - **Stabilize demand with a clear promo & campaign calendar**\n",
            "    - The quarter is volatile: strong growth into Feb, then a steep drop in March.\n",
            "    - Action:\n",
            "      - Build a **monthly campaign calendar** to smooth out sales: lighter but consistent promotions in ‚Äúweak‚Äù months, and stronger pushes around known high-demand periods.\n",
            "      - Use **retention tactics** (email, remarketing, loyalty offers) at the end of strong months (like Feb) to pull those customers back in March and reduce the drop-off.\n",
            "\n",
            "  - **Optional deeper dive (if data available)**\n",
            "    - Segment by channel, product category, or customer type to see **where** the March drop is concentrated.\n",
            "    - If the decline is channel-specific (e.g., paid ads, marketplace), reallocate budget toward the channels that behaved more like February.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ GOOD: Specific, detailed user prompt with context\n",
        "\n",
        "messages_good = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"\n",
        "Analyze the following quarterly sales data for Q1 2024:\n",
        "\n",
        "Month | Revenue | Units Sold | Avg Order Value\n",
        "Jan   | $45,000 | 1,200      | $37.50\n",
        "Feb   | $52,000 | 1,450      | $35.86\n",
        "Mar   | $38,000 | 980        | $38.78\n",
        "\n",
        "Tasks:\n",
        "1. Calculate the month-over-month growth rate\n",
        "2. Identify the best and worst performing months\n",
        "3. Provide 2-3 actionable insights based on trends\n",
        "4. Format your response as bullet points\n",
        "\n",
        "Focus on practical business implications, not just numbers.\n",
        "\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "response_good = get_completion_from_messages(client, messages_good)\n",
        "print(\"‚úÖ GOOD RESPONSE (specific task with data):\")\n",
        "print(response_good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example 3: Code Generation (Combined System + User Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå BAD RESPONSE (unclear requirements):\n",
            "Here‚Äôs a simple example in Python that sorts a list in ascending order without using the built-in `sort()` or `sorted()` (using a basic implementation of bubble sort):\n",
            "\n",
            "```python\n",
            "def sort_list(nums):\n",
            "    \"\"\"\n",
            "    Sort a list of numbers in ascending order using bubble sort.\n",
            "    Returns a new sorted list.\n",
            "    \"\"\"\n",
            "    arr = nums[:]  # make a copy so we don't modify the original\n",
            "    n = len(arr)\n",
            "    \n",
            "    for i in range(n):\n",
            "        # Last i elements are already in place\n",
            "        for j in range(0, n - i - 1):\n",
            "            if arr[j] > arr[j + 1]:\n",
            "                # swap\n",
            "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
            "    return arr\n",
            "\n",
            "# Example usage:\n",
            "data = [5, 2, 9, 1, 5, 6]\n",
            "print(sort_list(data))  # [1, 2, 5, 5, 6, 9]\n",
            "```\n",
            "\n",
            "If you want a version that just uses Python‚Äôs built-in sorting:\n",
            "\n",
            "```python\n",
            "def sort_list(nums):\n",
            "    return sorted(nums)\n",
            "```\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚ùå BAD: No clear guidelines\n",
        "\n",
        "messages_bad = [\n",
        "    {\"role\": \"system\", \"content\": \"You write code.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to sort a list.\"},\n",
        "]\n",
        "\n",
        "response_bad = get_completion_from_messages(client, messages_bad)\n",
        "print(\"‚ùå BAD RESPONSE (unclear requirements):\")\n",
        "print(response_bad)\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GOOD RESPONSE (well-defined requirements):\n",
            "```python\n",
            "from typing import List, Dict, Any\n",
            "\n",
            "\n",
            "def get_top_scorers(items: List[Dict[str, Any]], n: int) -> List[Dict[str, Any]]:\n",
            "    \"\"\"Return the top N items sorted by score in descending order.\n",
            "\n",
            "    Each item in `items` must be a dictionary containing:\n",
            "      - 'name': str\n",
            "      - 'score': int\n",
            "\n",
            "    If `n` is greater than the number of items, all items are returned.\n",
            "    If the list is empty or `n` <= 0, an empty list is returned.\n",
            "\n",
            "    Args:\n",
            "        items: List of dictionaries with 'name' and 'score' keys.\n",
            "        n: Number of top items to return.\n",
            "\n",
            "    Returns:\n",
            "        A list of up to N dictionaries sorted by 'score' in descending order.\n",
            "\n",
            "    Raises:\n",
            "        TypeError: If `items` is not a list or `n` is not an int.\n",
            "        ValueError: If an item is missing required keys or has invalid types.\n",
            "\n",
            "    Examples:\n",
            "        >>> data = [\n",
            "        ...     {'name': 'Alice', 'score': 95},\n",
            "        ...     {'name': 'Bob', 'score': 87},\n",
            "        ...     {'name': 'Charlie', 'score': 92},\n",
            "        ... ]\n",
            "        >>> get_top_scorers(data, 2)\n",
            "        [{'name': 'Alice', 'score': 95}, {'name': 'Charlie', 'score': 92}]\n",
            "\n",
            "        >>> get_top_scorers(data, 10)  # N > list length\n",
            "        [{'name': 'Alice', 'score': 95},\n",
            "         {'name': 'Charlie', 'score': 92},\n",
            "         {'name': 'Bob', 'score': 87}]\n",
            "\n",
            "        >>> get_top_scorers([], 3)\n",
            "        []\n",
            "\n",
            "    \"\"\"\n",
            "    if not isinstance(items, list):\n",
            "        raise TypeError(\"items must be a list of dictionaries\")\n",
            "\n",
            "    if not isinstance(n, int):\n",
            "        raise TypeError(\"n must be an integer\")\n",
            "\n",
            "    if n <= 0 or not items:\n",
            "        return []\n",
            "\n",
            "    # Validate items\n",
            "    for idx, item in enumerate(items):\n",
            "        if not isinstance(item, dict):\n",
            "            raise ValueError(f\"Item at index {idx} is not a dictionary\")\n",
            "        if \"name\" not in item or \"score\" not in item:\n",
            "            raise ValueError(\n",
            "                f\"Item at index {idx} must contain 'name' and 'score' keys\"\n",
            "            )\n",
            "        if not isinstance(item[\"name\"], str):\n",
            "            raise ValueError(f\"'name' at index {idx} must be a string\")\n",
            "        if not isinstance(item[\"score\"], int):\n",
            "            raise ValueError(f\"'score' at index {idx} must be an integer\")\n",
            "\n",
            "    # Sort by score descending\n",
            "    sorted_items = sorted(items, key=lambda x: x[\"score\"], reverse=True)\n",
            "\n",
            "    # Return top N (handles N > len(items) naturally)\n",
            "    return sorted_items[:n]\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    example_input = [\n",
            "        {\"name\": \"Alice\", \"score\": 95},\n",
            "        {\"name\": \"Bob\", \"score\": 87},\n",
            "    ]\n",
            "    print(get_top_scorers(example_input, 1))\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ GOOD: Clear system guidelines + specific user requirements\n",
        "\n",
        "messages_good = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "You are a senior Python developer specializing in clean, production-ready code.\n",
        "\n",
        "Code style:\n",
        "- Use type hints for all functions\n",
        "- Include docstrings (Google style)\n",
        "- Add error handling where appropriate\n",
        "- Prefer readability over cleverness\n",
        "- Include usage examples in docstrings\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"\n",
        "Write a Python function that:\n",
        "- Accepts a list of dictionaries\n",
        "- Each dictionary has 'name' (str) and 'score' (int) keys\n",
        "- Sorts by score in descending order\n",
        "- Returns the top N items (N is a parameter)\n",
        "- Handles edge cases (empty list, N > list length)\n",
        "\n",
        "Example input: [{'name': 'Alice', 'score': 95}, {'name': 'Bob', 'score': 87}]\n",
        "\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "response_good = get_completion_from_messages(client, messages_good)\n",
        "print(\"‚úÖ GOOD RESPONSE (well-defined requirements):\")\n",
        "print(response_good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterative Prompt Improvement Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Improving prompts is an iterative process that requires systematic testing and refinement. Rather than guessing what might work, you can use structured approaches‚Äîincluding AI-powered tools‚Äîto optimize your prompts.\n",
        "\n",
        "**The Manual Approach: Asking an AI to Improve Your Prompt**\n",
        "\n",
        "Before using specialized tools, you can leverage any LLM to help improve your prompts. Here's a structured way to ask for prompt improvements:\n",
        "\n",
        "**Template for requesting prompt improvements:**\n",
        "\n",
        "```\n",
        "I have the following prompt that I'm using for [describe use case]:\n",
        "\n",
        "[Your current prompt]\n",
        "\n",
        "Please analyze this prompt and suggest improvements focusing on:\n",
        "1. Clarity - Is the instruction clear and unambiguous?\n",
        "2. Completeness - Are there missing edge cases or instructions?\n",
        "3. Structure - Is it well-organized and easy to follow?\n",
        "4. Specificity - Are the requirements specific enough?\n",
        "5. Actionability - Can the model easily act on these instructions?\n",
        "\n",
        "Provide:\n",
        "- A quality assessment of the current prompt\n",
        "- Specific issues identified\n",
        "- An improved version with explanations for each change\n",
        "```\n",
        "\n",
        "**Example conversation with an AI:**\n",
        "\n",
        "**You:** \"Analyze this prompt for a customer support bot: 'You are helpful. Answer user questions.' Suggest improvements.\"\n",
        "\n",
        "**AI:** \"Current issues: Too vague, no guidelines, no edge case handling. Improved version: 'You are a customer support assistant for [Company]. Guidelines: 1) Be empathetic and professional, 2) Provide step-by-step solutions, 3) Escalate to humans if you cannot resolve the issue, 4) Never ask for passwords or sensitive data.'\"\n",
        "\n",
        "While this manual approach works, it requires you to:\n",
        "- Craft good meta-prompts (prompts about prompts)\n",
        "- Manually track quality metrics\n",
        "- Iteratively test different versions\n",
        "- Document what works and what doesn't\n",
        "\n",
        "This is where specialized tools like **Clavix** become valuable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clavix: An AI-Powered Prompt Optimization Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What is Clavix?**\n",
        "\n",
        "[Clavix](https://clavix.dev/) is an open-source framework designed to systematically improve prompts through structured analysis and pattern-based optimization. Unlike ad-hoc manual improvements, Clavix provides:\n",
        "\n",
        "- **Standardized Quality Assessment**: Evaluates prompts across 6 dimensions\n",
        "- **Automated Depth Selection**: Chooses the appropriate level of improvement based on current quality\n",
        "- **Pattern-Based Improvements**: Applies proven optimization patterns systematically\n",
        "- **Measurable Results**: Provides before/after quality scores to track improvements\n",
        "- **Reproducible Process**: Uses a consistent methodology for reliable results\n",
        "\n",
        "**Why Use Clavix?**\n",
        "\n",
        "1. **Objective Evaluation**: Removes subjectivity from prompt assessment\n",
        "2. **Systematic Approach**: Follows a structured methodology rather than random tweaking\n",
        "3. **Learning Tool**: Helps you understand what makes prompts effective\n",
        "4. **Time Efficiency**: Automates the analysis and improvement process\n",
        "5. **Documentation**: Automatically tracks changes and rationale\n",
        "\n",
        "**The 6 Quality Dimensions**\n",
        "\n",
        "Clavix evaluates prompts across these dimensions:\n",
        "\n",
        "| Dimension | What It Measures | Example Issues |\n",
        "|-----------|------------------|----------------|\n",
        "| **Clarity** | How clear and unambiguous the instructions are | Vague language, conflicting requirements |\n",
        "| **Efficiency** | Token usage vs. value delivered | Unnecessary verbosity, redundancy |\n",
        "| **Structure** | Logical organization and readability | Wall of text, poor formatting |\n",
        "| **Completeness** | Coverage of edge cases and requirements | Missing error handling, undefined behavior |\n",
        "| **Actionability** | How easily the model can execute the task | Abstract goals without concrete steps |\n",
        "| **Specificity** | Level of detail and precision | Generic instructions, unclear constraints |\n",
        "\n",
        "**How Clavix Works**\n",
        "\n",
        "1. **Intent Detection**: Identifies the prompt's purpose (e.g., code-generation, creative-writing, analysis)\n",
        "2. **Quality Assessment**: Scores each dimension (0-100%)\n",
        "3. **Depth Selection**: Automatically chooses improvement level (QUICK, STANDARD, DEEP) based on score\n",
        "4. **Pattern Application**: Applies relevant improvement patterns (e.g., STRUCTURED, CLARIFIED, EXPANDED)\n",
        "5. **Optimization**: Generates an improved version with measurable quality gains\n",
        "6. **Reporting**: Provides detailed before/after analysis\n",
        "\n",
        "**Links:**\n",
        "- Homepage: https://clavix.dev/\n",
        "- GitHub Repository: https://github.com/ClavixDev/Clavix\n",
        "- Documentation: Available in the repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing and Using Clavix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Installation**\n",
        "\n",
        "Clavix can be installed via npm (Node Package Manager). You need to have Node.js installed on your system first.\n",
        "\n",
        "Install Clavix globally (run this in your terminal, not in the notebook)\n",
        "\n",
        "```\n",
        "npm install -g clavix\n",
        "```\n",
        "\n",
        "Or use npx to run without installation:\n",
        "```\n",
        "npx clavix init\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Initialization**\n",
        "\n",
        "Before using Clavix, initialize it in your project directory to set up configuration:\n",
        "\n",
        "Run in terminal (not in notebook):\n",
        "\n",
        "``` bash\n",
        "cd /path/to/your/project\n",
        "clavix init\n",
        "```\n",
        "\n",
        "You will be prompted to select your AI model and set up configuration. This creates a .clavix directory with configuration files. You can now use Clavix through your AI assistant or command line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Basic Usage Workflow**\n",
        "\n",
        "Clavix can be used in two main ways:\n",
        "\n",
        "1. **Through an AI Assistant** (recommended for interactive work):\n",
        "   - Ask your AI: \"Use Clavix to improve this prompt: [your prompt]\"\n",
        "   - The AI will run Clavix's improvement workflow automatically\n",
        "\n",
        "2. **Command Line Interface**:\n",
        "   - Direct commands for batch processing or automation\n",
        "   - Useful for CI/CD pipelines or testing multiple prompts\n",
        "\n",
        "For this notebook, we'll demonstrate the AI assistant approach, which is more interactive and educational."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Example: Improving the RAG Q&A Prompt with Clavix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take the RAG Q&A system prompt from Section 2 and improve it using Clavix. We'll walk through the complete improvement process.\n",
        "\n",
        "**Original Prompt** (from the `answer_question` function in Section 2):\n",
        "\n",
        "```\n",
        "You are a question-answering assistant.\n",
        "Answer the question using ONLY the provided context.\n",
        "If the answer is not in the context, say 'I don't know'.\n",
        "```\n",
        "\n",
        "This prompt works, but let's see how Clavix can make it better by calling Clavix through our AI assistant:\n",
        "\n",
        "```\n",
        "/clavix-improve the RAG Q&A system prompt in the Jupyter notebook to enhance clarity, completeness, and actionability.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Intent Detection\n",
        "\n",
        "**Detected Intent:** `code-generation` (RAG retrieval-augmented generation pattern)\n",
        "\n",
        "**Context:** System prompt for a RAG pipeline that answers questions using PDF document chunks. The LLM must ground responses in retrieved context only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Quality Assessment (6 Dimensions)\n",
        "\n",
        "| Dimension | Score | Assessment |\n",
        "|-----------|-------|------------|\n",
        "| **Clarity** | 65% | Objective is clear but lacks specificity about HOW to use context |\n",
        "| **Efficiency** | 80% | Concise, but missing critical instruction details |\n",
        "| **Structure** | 50% | Single paragraph, lacks logical organization |\n",
        "| **Completeness** | 45% | Missing: citation requirements, confidence levels, partial answers |\n",
        "| **Actionability** | 60% | Basic action defined but lacks handling edge cases |\n",
        "| **Specificity** | 55% | Generic \"assistant\" role, no domain context |\n",
        "\n",
        "**Overall Quality:** 59%\n",
        "\n",
        "**Auto-selected Depth:** STANDARD (score < 60% - needs basic fixes first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Improvement Patterns Applied\n",
        "\n",
        "Clavix applies these patterns to address the identified issues:\n",
        "\n",
        "- **[STRUCTURED]** - Added role definition and task breakdown\n",
        "- **[CLARIFIED]** - Explicit instructions for context usage\n",
        "- **[EXPANDED]** - Added handling for partial matches and confidence\n",
        "- **[SCOPED]** - Clear boundaries for out-of-scope questions\n",
        "- **[ADDED]** - Citation requirements for transparency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Optimized Prompt\n",
        "\n",
        "Here's the improved version generated by Clavix:\n",
        "\n",
        "```\n",
        "You are a document-based question-answering assistant specializing in retrieval-augmented generation (RAG).\n",
        "\n",
        "Your task:\n",
        "1. Carefully read the provided context from retrieved document chunks\n",
        "2. Answer the user's question using ONLY information found in this context\n",
        "3. Quote or paraphrase relevant portions when possible for transparency\n",
        "4. If the context contains a partial answer, provide what you can and acknowledge what's missing\n",
        "\n",
        "Critical rules:\n",
        "- Never use external knowledge or training data - context is your only source\n",
        "- If the answer is not in the context, respond with: \"I don't know - this information is not available in the provided documents\"\n",
        "- If you're uncertain, acknowledge it: \"Based on the available context, it appears that...\"\n",
        "- Do not make assumptions or inferences beyond what the context explicitly states\n",
        "\n",
        "Output format:\n",
        "- Provide direct, concise answers\n",
        "- Cite specific context when possible (\"According to the document...\")\n",
        "- Maintain a helpful, professional tone\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 5: Quality Improvement Summary\n",
        "\n",
        "| Dimension | Before | After | Improvement |\n",
        "|-----------|--------|-------|-------------|\n",
        "| **Clarity** | 65% | 85% | +20% - Clear role, numbered steps |\n",
        "| **Efficiency** | 80% | 75% | -5% - Slightly longer but necessary detail |\n",
        "| **Structure** | 50% | 90% | +40% - Organized sections with headers |\n",
        "| **Completeness** | 45% | 85% | +40% - Added edge cases, citation guidance |\n",
        "| **Actionability** | 60% | 85% | +25% - Explicit step-by-step process |\n",
        "| **Specificity** | 55% | 80% | +25% - Domain-specific RAG context |\n",
        "\n",
        "**New Overall Quality:** 83% (+24% improvement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 6: Key Improvements Explained\n",
        "\n",
        "1. **Role Clarity**: Changed from generic \"assistant\" to \"document-based question-answering assistant specializing in RAG\"\n",
        "   - *Why it matters:* Specific roles help the model understand its constraints and purpose\n",
        "\n",
        "2. **Process Definition**: Added numbered steps for systematic approach\n",
        "   - *Why it matters:* Explicit steps reduce ambiguity and ensure consistent behavior\n",
        "\n",
        "3. **Edge Case Handling**: Instructions for partial matches, not just binary yes/no\n",
        "   - *Why it matters:* Real-world queries often don't have perfect matches in the context\n",
        "\n",
        "4. **Citation Guidance**: Encourages quoting/paraphrasing for transparency\n",
        "   - *Why it matters:* Users can verify answers and understand confidence levels\n",
        "\n",
        "5. **Confidence Calibration**: Acknowledges uncertainty when appropriate\n",
        "   - *Why it matters:* Prevents hallucinations and builds user trust\n",
        "\n",
        "6. **Boundary Enforcement**: Multiple reminders about context-only usage\n",
        "   - *Why it matters:* Critical for RAG systems to avoid mixing retrieved and parametric knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the Improved Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the original and improved prompts side-by-side using the same RAG pipeline from Section 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original answer_question function (from Section 2)\n",
        "def answer_question_original(question, context_chunks):\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "          You are a question-answering assistant.\n",
        "          Answer the question using ONLY the provided context.\n",
        "          If the answer is not in the context, say 'I don't know'.\n",
        "       \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "          Context: {context}\n",
        "          Question: {question}\n",
        "       \"\"\",\n",
        "        },\n",
        "    ]\n",
        "    return get_completion_from_messages(client, messages)\n",
        "\n",
        "\n",
        "# Improved answer_question function (optimized by Clavix)\n",
        "def answer_question_improved(question, context_chunks):\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "You are a document-based question-answering assistant specializing in retrieval-augmented generation (RAG).\n",
        "\n",
        "Your task:\n",
        "1. Carefully read the provided context from retrieved document chunks\n",
        "2. Answer the user's question using ONLY information found in this context\n",
        "3. Quote or paraphrase relevant portions when possible for transparency\n",
        "4. If the context contains a partial answer, provide what you can and acknowledge what's missing\n",
        "\n",
        "Critical rules:\n",
        "- Never use external knowledge or training data - context is your only source\n",
        "- If the answer is not in the context, respond with: \"I don't know - this information is not available in the provided documents\"\n",
        "- If you're uncertain, acknowledge it: \"Based on the available context, it appears that...\"\n",
        "- Do not make assumptions or inferences beyond what the context explicitly states\n",
        "\n",
        "Output format:\n",
        "- Provide direct, concise answers\n",
        "- Cite specific context when possible (\"According to the document...\")\n",
        "- Maintain a helpful, professional tone\n",
        "       \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "          Context: {context}\n",
        "          Question: {question}\n",
        "       \"\"\",\n",
        "        },\n",
        "    ]\n",
        "    return get_completion_from_messages(client, messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ORIGINAL PROMPT RESPONSE:\n",
            "================================================================================\n",
            "I don't know.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "IMPROVED PROMPT RESPONSE:\n",
            "================================================================================\n",
            "I don't know - this information is not available in the provided documents.\n",
            "\n",
            "Based on the context, the document only states that the company‚Äôs activity code is ‚Äú6429 Financi√´le holdings‚Äù and that it is a ‚ÄúBesloten Vennootschap‚Äù (private limited company), but it does not describe any specific services or activities beyond this classification.\n"
          ]
        }
      ],
      "source": [
        "question = (\n",
        "    \"Summarize the main services offered by the company described in the document.\"\n",
        ")\n",
        "\n",
        "document_text = load_pdf(\"KvK_1_Mockup.pdf\")\n",
        "chunks = chunk_text(document_text)\n",
        "chunk_embeddings = [get_embedding(chunk) for chunk in chunks]\n",
        "relevant_chunks = retrieve_relevant_chunks(question, chunks, chunk_embeddings)\n",
        "\n",
        "# Compare responses\n",
        "print(\"=\" * 80)\n",
        "print(\"ORIGINAL PROMPT RESPONSE:\")\n",
        "print(\"=\" * 80)\n",
        "answer_original = answer_question_original(question, relevant_chunks)\n",
        "print(answer_original)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"IMPROVED PROMPT RESPONSE:\")\n",
        "print(\"=\" * 80)\n",
        "answer_improved = answer_question_improved(question, relevant_chunks)\n",
        "print(answer_improved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Differences:**\n",
        "\n",
        "The improved prompt should produce responses that:\n",
        "1. **Are more transparent**: \"According to the document...\" vs just stating facts\n",
        "2. **Handle uncertainty better**: Explicitly acknowledges when information is partial\n",
        "3. **Are more trustworthy**: Cites sources and avoids hallucinations\n",
        "4. **Follow consistent structure**: Maintains professional tone across all answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Use Clavix for Your Own Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**General Workflow:**\n",
        "\n",
        "1. **Identify a prompt to improve**\n",
        "   - Can be from this notebook or your own projects\n",
        "   - Should be a complete system or user prompt\n",
        "\n",
        "2. **Request Clavix improvement** (through an AI assistant like GitHub Copilot)\n",
        "   - Example: \"Use Clavix to improve this prompt: [paste your prompt]\"\n",
        "   - Or: \"Analyze this prompt with Clavix and suggest improvements\"\n",
        "\n",
        "3. **Review the analysis**\n",
        "   - Check the 6-dimension quality scores\n",
        "   - Understand which patterns were applied\n",
        "   - Read the explanations for each improvement\n",
        "\n",
        "4. **Test both versions**\n",
        "   - Run your original prompt with test inputs\n",
        "   - Run the improved prompt with the same inputs\n",
        "   - Compare results objectively\n",
        "\n",
        "5. **Measure improvement** (optional but recommended)\n",
        "   - Use LLM-as-a-Judge (from Section 2) to evaluate both versions\n",
        "   - Calculate accuracy, relevance, or other metrics\n",
        "   - Document which version performs better\n",
        "\n",
        "6. **Iterate if needed**\n",
        "   - If quality is still below 80%, request further improvements\n",
        "   - Focus on specific dimensions that scored low\n",
        "   - Test with edge cases\n",
        "\n",
        "**Example Request Formats:**\n",
        "\n",
        "```\n",
        "\"Use Clavix to improve the movie recommender system prompt from Section 1\"\n",
        "\n",
        "\"Analyze this prompt with Clavix: [prompt text]\"\n",
        "\n",
        "\"Run Clavix improvement workflow on the customer support prompt\"\n",
        "\n",
        "\"Use Clavix to optimize this code generation prompt for better clarity and structure\"\n",
        "```\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "- Start with STANDARD depth for most prompts (let Clavix auto-select)\n",
        "- Focus on one prompt at a time for clarity\n",
        "- Test improvements with real use cases, not just examples\n",
        "- Document the before/after scores for your records\n",
        "- Share improved prompts with your team"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7MeTaOl24xJB",
        "MQW6YomX5TD8",
        "AjawOuB6tFI0",
        "54RTZJkttOev",
        "w2RU6MiPtXA3",
        "9oEHKeESCGK8",
        "OJ9P-waMxFR_",
        "XOBOSfFZxMZS",
        "Ult71EJk9xv_",
        "9O7pk4vD0HPw",
        "4v9dWcpe0V6W"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
